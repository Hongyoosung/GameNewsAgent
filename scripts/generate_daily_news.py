import os
import sys
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ìƒˆë¡œìš´ ê³µì‹ SDK ì‚¬ìš©
from google import genai
from google.genai import types

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
TARGET_REPO_PATH = os.environ.get("TARGET_REPO_PATH", ".")
KST = timezone(timedelta(hours=9))
TODAY = datetime.now(KST)
TODAY_STR = TODAY.strftime("%Y-%m-%d")

if not GEMINI_API_KEY:
    print("ğŸš¨ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# ìµœì‹  genai í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = genai.Client(api_key=GEMINI_API_KEY)
MODEL_ID = 'gemini-2.5-flash'

# ì•ˆì „ í•„í„° ì™„í™” (ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ì‹œ ì˜¤íƒì§€ ë°©ì§€ - ìƒˆë¡œìš´ SDK ë°©ì‹)
safety_settings = [
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
]

def call_gemini(prompt: str, is_json=False) -> str:
    """ê²°ì œ ê³„ì • ì—°ë™ ìƒíƒœì´ë¯€ë¡œ, ëŒ€ê¸°ì—´(Sleep) ì—†ì´ ì¦‰ê° í˜¸ì¶œí•©ë‹ˆë‹¤."""
    config_args = {"safety_settings": safety_settings}
    if is_json:
        config_args["response_mime_type"] = "application/json"
        
    config = types.GenerateContentConfig(**config_args)
    
    try:
        response = client.models.generate_content(
            model=MODEL_ID,
            contents=prompt,
            config=config
        )
        return response.text
    except Exception as e:
        print(f"    âš ï¸ Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        raise

def fetch_recent_rss_entries() -> list:
    """ìµœê·¼ 24ì‹œê°„ ì´ë‚´ì˜ RSS í”¼ë“œ ìˆ˜ì§‘"""
    urls = [
        "https://news.ycombinator.com/rss",
        "https://www.reddit.com/r/MachineLearning/new/.rss"
    ]
    yesterday = TODAY - timedelta(days=1)
    entries = []

    for url in urls:
        print(f"  ğŸ“¥ RSS íŒŒì‹± ì¤‘: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            published_tuple = entry.get('published_parsed', entry.get('updated_parsed'))
            if published_tuple:
                published_dt = datetime(*published_tuple[:6], tzinfo=timezone.utc)
                if published_dt > yesterday:
                    entries.append({
                        "title": entry.title,
                        "link": entry.link,
                        # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ë¹„í•˜ì—¬ RSS ë‚´ ìš”ì•½ë³¸ë„ ìˆ˜ì§‘ (ìµœëŒ€ 500ì)
                        "summary": entry.get('summary', '')[:500] 
                    })
    return entries

def extract_webpage_text(url: str) -> str:
    """URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Reddit 403 ë°©ì§€ë¥¼ ìœ„í•´ User-Agent ê°•í™”)"""
    try:
        # ì¼ë°˜ í¬ë¡¬ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.extract()
            
        text = soup.get_text(separator=' ', strip=True)
        return text[:3000]
    except Exception as e:
        print(f"    âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (RSS ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´ë¨): {e}")
        return ""

def main():
    print(f"ğŸš€ [1/4] RSS í”¼ë“œ ìˆ˜ì§‘ ë° ê¸°ì‚¬ ì„ ë³„ ì‹œì‘...")
    rss_entries = fetch_recent_rss_entries()
    
    if not rss_entries:
        print("ğŸš¨ ìµœê·¼ 24ì‹œê°„ ë‚´ì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # ìš”ì²­ ì‚¬í•­: ìµœëŒ€ 5ê°œ ê¸°ì‚¬ ì„ ë³„
    rss_text = "\n".join([f"- ì œëª©: {e['title']}\n  ë§í¬: {e['link']}\n  ìš”ì•½: {e['summary']}" for e in rss_entries])
    step1_prompt = f"""
    ë‹¤ìŒì€ ìµœê·¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ ì¤‘ì—ì„œ 'ê²Œì„ í”„ë¡œê·¸ë˜ë°' ë° 'AI/ML ê¸°ìˆ 'ê³¼ ê´€ë ¨ëœ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì‚¬ë¥¼ **ìµœëŒ€ 5ê°œ** ì„ ë³„í•´ì£¼ì„¸ìš”.
    Unreal/Unity ì—…ë°ì´íŠ¸, LLM ë…¼ë¬¸, ê·¸ë˜í”½ìŠ¤ ìµœì í™” ë“± ê¸°ìˆ  ì¤‘ì‹¬ì´ì–´ì•¼ í•˜ë©°, ë‹¨ìˆœ ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ê²Œì„ ì¶œì‹œ ì†Œì‹ì€ ì œì™¸í•˜ì„¸ìš”.
    
    ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì˜ ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
    [
      {{"title": "ê¸°ì‚¬ ì œëª©", "link": "ê¸°ì‚¬ URL", "rss_summary": "ìˆ˜ì§‘ëœ ìš”ì•½ ë‚´ìš©"}}, ...
    ]
    
    ê¸°ì‚¬ ëª©ë¡:
    {rss_text}
    """
    selected_links_json = call_gemini(step1_prompt, is_json=True)
    selected_articles = json.loads(selected_links_json)
    print(f"    âœ… {len(selected_articles)}ê°œì˜ ê¸°ì‚¬ ì„ ë³„ ì™„ë£Œ.")

    print(f"ğŸš€ [2/4] ì„ ë³„ëœ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½...")
    summaries = []
    for idx, article in enumerate(selected_articles):
        print(f"    ğŸ“– ë¶„ì„ ì¤‘ ({idx+1}/{len(selected_articles)}): {article['title']}")
        content = extract_webpage_text(article['link'])
        
        # ë³¸ë¬¸(content)ì´ 403 ì—ëŸ¬ë¡œ ë¹„ì–´ìˆë”ë¼ë„, RSS ìì²´ ìš”ì•½(rss_summary)ì„ ì£¼ì–´ ìœ ì¶”í•˜ê²Œ í•¨
        step2_prompt = f"""
        ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        
        ì œëª©: {article['title']}
        ë§í¬: {article['link']}
        RSS ê¸°ë³¸ ìš”ì•½: {article.get('rss_summary', '')}
        ë³¸ë¬¸ ë‚´ìš©: {content if content else "(ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ë§í¬, RSS ê¸°ë³¸ ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚´ìš©ì„ ìœ ì¶”í•˜ì„¸ìš”.)"}
        
        í˜•ì‹:
        #### ê¸°ì‚¬
        ë§í¬: [{article['title']}]({article['link']})
        ìš”ì•½: (í•µì‹¬ ê¸°ìˆ  ë‚´ìš© 1ì¤„)
        ì˜í–¥: (ê²Œì„/AI ê°œë°œ ì˜í–¥ 1ì¤„)
        """
        summary = call_gemini(step2_prompt)
        summaries.append(summary)
    
    print(f"ğŸš€ [3/4] ìµœì¢… ë§ˆí¬ë‹¤ìš´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±...")
    combined_summaries = "\n\n".join(summaries)
    step3_prompt = f"""
    ì˜¤ëŠ˜ ë‚ ì§œëŠ” {TODAY_STR}ì…ë‹ˆë‹¤.
    
    ë‹¤ìŒ ìš”ì•½ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    íƒ€ê¹ƒ ë…ì: 'ê²Œì„ í´ë¼ì´ì–¸íŠ¸ í”„ë¡œê·¸ë˜ë¨¸' ë° 'AI ì—”ì§€ë‹ˆì–´'.
    
    [ì¶œë ¥ í˜•ì‹]
    ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ìš© ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ë§Œ ì¶œë ¥. ì¶”ê°€ ì„¤ëª…/ì½”ë“œë¸”ë¡(```markdown ë“±) ê¸ˆì§€.
    ì‹¤ì œ ê¸°ì‚¬ URL ë§í¬ í•„ìˆ˜.
    
    ---
    title: "[ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ ì œëª© ì‘ì„± - ì˜ˆ: {TODAY_STR} Unreal C++ ìµœì í™” & LLM íŠ¸ë Œë“œ]"
    date: {TODAY.strftime("%Y-%m-%dT09:00:00+09:00")}
    draft: false
    description: "[í•µì‹¬ ê¸°ìˆ  ë™í–¥ 2-3ì¤„ ìš”ì•½ - ê²Œì„ ê°œë°œ/AI ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸ ì¤‘ì‹¬]"
    tags: ["News", "Game Programming", "AI Trends", "Tech"]
    categories: ["Tech"]
    ---
    
    ìµœì‹  ê²Œì„ í”„ë¡œê·¸ë˜ë° ë° AI ê¸°ìˆ  ë™í–¥ì„ ì „í•´ë“œë¦½ë‹ˆë‹¤.
    
    (ì´í›„ ê° ê¸°ì‚¬ë³„ë¡œ ì•„ë˜ í˜•ì‹ ìœ ì§€)
    #### 1. [ì‹¤ì œ ê¸°ì‚¬ ì œëª©](ì‹¤ì œ ë§í¬ URL)
    * **í•µì‹¬ ë‚´ìš©:** ...
    * **ê¸°ìˆ ì  ì˜ë¯¸:** ...
    * **í™œìš© ë°©ì•ˆ:** ...
    
    [ìš”ì•½ ë°ì´í„°]
    {combined_summaries}
    """
    
    final_markdown = call_gemini(step3_prompt)
    final_markdown = final_markdown.replace("```markdown\n", "").replace("```\n", "").strip()

    print(f"ğŸš€ [4/4] íŒŒì¼ ì €ì¥ ì¤‘...")
    target_dir = os.path.join(TARGET_REPO_PATH, "content", "journal")
    os.makedirs(target_dir, exist_ok=True)
    
    file_name = f"{TODAY_STR}_news.ko.md"
    file_path = os.path.join(target_dir, file_name)
    
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(final_markdown)
        
    print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")

if __name__ == "__main__":
    main()