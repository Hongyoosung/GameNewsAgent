import os
import sys
import json
import requests
import feedparser
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ìƒˆë¡œìš´ ê³µì‹ SDK ì‚¬ìš©
from google import genai
from google.genai import types

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
TARGET_REPO_PATH = os.environ.get("TARGET_REPO_PATH", ".")
KST = timezone(timedelta(hours=9))
TODAY = datetime.now(KST)
TODAY_STR = TODAY.strftime("%Y-%m-%d")

if not GEMINI_API_KEY:
    print("ğŸš¨ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

# ìµœì‹  genai í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = genai.Client(api_key=GEMINI_API_KEY)
MODEL_ID = 'gemini-2.5-flash'

# ì•ˆì „ í•„í„° ì™„í™” (ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ì‹œ ì˜¤íƒì§€ ë°©ì§€ - ìƒˆë¡œìš´ SDK ë°©ì‹)
safety_settings = [
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
    types.SafetySetting(category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=types.HarmBlockThreshold.BLOCK_NONE),
]

def clean_generated_text(text: str) -> str:
    """AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ê¹¨ì§„ ë§ˆí¬ë‹¤ìš´ì„ ë³µêµ¬í•©ë‹ˆë‹¤."""
    if not text:
        return text
        
    # 1. [P], [R] ë“±ì˜ ë§ë¨¸ë¦¬ íƒœê·¸ ì œê±°
    cleaned_text = re.sub(r'\[[A-Z]{1,2}\]\s*', '', text)
    
    # 2. (2 lines) ê°™ì€ ê°€ì´ë“œ ë¬¸êµ¬ ì œê±°
    cleaned_text = re.sub(r'\s*\(\d+\s*lines?\)', '', cleaned_text)
    
    # 3. ê¹¨ì§„ ë§ˆí¬ë‹¤ìš´ ë§í¬ ìë™ ë³µêµ¬ 
    cleaned_text = re.sub(
        r'^(?:###\s*)?(\d+)\.\s*([^\[\n]+?)\s*\((https?://[^\)]+)\)',
        r'### \1. [\2](\3)',
        cleaned_text,
        flags=re.MULTILINE
    )
    
    return cleaned_text

def call_gemini(prompt: str, is_json=False) -> str:
    """ê²°ì œ ê³„ì • ì—°ë™ ìƒíƒœì´ë¯€ë¡œ, ëŒ€ê¸°ì—´(Sleep) ì—†ì´ ì¦‰ê° í˜¸ì¶œí•©ë‹ˆë‹¤."""
    config_args = {"safety_settings": safety_settings}
    if is_json:
        config_args["response_mime_type"] = "application/json"
        
    config = types.GenerateContentConfig(**config_args)
    
    try:
        response = client.models.generate_content(
            model=MODEL_ID,
            contents=prompt,
            config=config
        )
        return response.text
    except Exception as e:
        print(f"    âš ï¸ Gemini API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        raise

def fetch_recent_rss_entries() -> list:
    """ìµœê·¼ 24ì‹œê°„ ì´ë‚´ì˜ RSS í”¼ë“œ ìˆ˜ì§‘"""
    urls = [
        "https://news.ycombinator.com/rss",
        "https://www.reddit.com/r/MachineLearning/new/.rss"
    ]
    yesterday = TODAY - timedelta(days=1)
    entries = []

    for url in urls:
        print(f"  ğŸ“¥ RSS íŒŒì‹± ì¤‘: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            published_tuple = entry.get('published_parsed', entry.get('updated_parsed'))
            if published_tuple:
                published_dt = datetime(*published_tuple[:6], tzinfo=timezone.utc)
                if published_dt > yesterday:
                    entries.append({
                        "title": entry.title,
                        "link": entry.link,
                        # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ë¹„í•˜ì—¬ RSS ë‚´ ìš”ì•½ë³¸ë„ ìˆ˜ì§‘ (ìµœëŒ€ 500ì)
                        "summary": entry.get('summary', '')[:500] 
                    })
    return entries

def extract_webpage_text(url: str) -> str:
    """URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Reddit 403 ë°©ì§€ë¥¼ ìœ„í•´ User-Agent ê°•í™”)"""
    try:
        # ì¼ë°˜ í¬ë¡¬ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.extract()
            
        text = soup.get_text(separator=' ', strip=True)
        return text[:3000]
    except Exception as e:
        print(f"    âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (RSS ìš”ì•½ë³¸ìœ¼ë¡œ ëŒ€ì²´ë¨): {e}")
        return ""

def main():
    print(f"ğŸš€ [1/5] RSS í”¼ë“œ ìˆ˜ì§‘ ë° ê¸°ì‚¬ ì„ ë³„ ì‹œì‘...")
    rss_entries = fetch_recent_rss_entries()
    
    if not rss_entries:
        print("ğŸš¨ ìµœê·¼ 24ì‹œê°„ ë‚´ì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # ìš”ì²­ ì‚¬í•­: ìµœëŒ€ 5ê°œ ê¸°ì‚¬ ì„ ë³„
    rss_text = "\n".join([f"- ì œëª©: {e['title']}\n  ë§í¬: {e['link']}\n  ìš”ì•½: {e['summary']}" for e in rss_entries])
    step1_prompt = f"""
    Below is a list of recently collected news articles.
    Please select **up to 5** of the most important articles related to "Game Programming" and "AI/ML Technology."
    These articles should be technically focused, such as Unreal/Unity updates, LLM theses, and graphics optimization, and should not include simple business or game release news.

    Please respond only as an array in JSON format:
    [
    {{"title": "Article Title", "link": "Article URL", "rss_summary": "Collected Summary"}}, ...
    ]

    Article List:
    {rss_text}
    """
    selected_links_json = call_gemini(step1_prompt, is_json=True)
    selected_articles = json.loads(selected_links_json)
    print(f"    âœ… {len(selected_articles)}ê°œì˜ ê¸°ì‚¬ ì„ ë³„ ì™„ë£Œ.")

    print(f"ğŸš€ [2/5] ì„ ë³„ëœ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½")
    summaries = []
    for idx, article in enumerate(selected_articles):
        print(f"    ğŸ“– ë¶„ì„ ì¤‘ ({idx+1}/{len(selected_articles)}): {article['title']}")
        content = extract_webpage_text(article['link'])
        
        step2_prompt = f"""
        Analyze the following article content and summarize it in English using the specified format.
        Focus strictly on the technical details relevant to game development and AI engineering.
        
        Title: {article['title']}
        Link: {article['link']}
        RSS Summary: {article.get('rss_summary', '')}
        Content: {content if content else "(Could not fetch content. Infer based on the title, link, and RSS summary.)"}
        
        Format:
        #### Article
        Link: [{article['title']}]({article['link']})
        Summary: (1 sentence of core technical content)
        Impact: (1 sentence on impact for Game/AI development)
        """
        summary = call_gemini(step2_prompt)
        summaries.append(summary)
    
    print(f"ğŸš€ [3/5] ìµœì¢… ë§ˆí¬ë‹¤ìš´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± (ì˜ë¬¸)...")
    combined_summaries = "\n\n".join(summaries)
    
    # ì¶œë ¥ í¬ë§·ì—ì„œ (2 lines) ë“±ì˜ ë¬¸êµ¬ ì œê±°, í”„ë¡¬í”„íŠ¸ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ê¸¸ì´ ì œí•œ ëª…ì‹œ
    step3_en_prompt = f"""
    Today's date is {TODAY_STR}.
    
    Based on the following summarized article data, write a final blog post in English.
    Target audience: 'Game Client Programmers' and 'AI Engineers'.
    
    [Output Format]
    Output ONLY the markdown body for the blog post. Do NOT include extra explanations or markdown code blocks (like ```markdown).
    MUST include actual article URL links.
    Remove any tags like [P], [R], [D] from the article titles.
    
    ---
    title: "[Write a catchy title based on the news - e.g., Unreal C++ Optimization & LLM Trends (Do not include date)]"
    date: {TODAY.strftime("%Y-%m-%dT09:00:00+09:00")}
    draft: false
    description: "[2-3 sentence summary of core tech trends - focus on game dev / AI practical applications]"
    tags: ["Tag1", "Tag2", "Tag3"] # Max 3 tags
    categories: ["Tech"]
    ---
    
    Here are the latest trends in game programming and AI technology.
    
    (Maintain the following format for each article. Keep the descriptions concise, around 2-3 sentences each)
    ### 1. [Actual Article Title Without Tags](Actual Link URL)
    * **Core Content:** ...
    * **Technical Significance:** ...
    * **Practical Application:** ...

    ---

    ### 2. [Actual Article Title Without Tags](Actual Link URL)
    
    [Summarized Data]
    {combined_summaries}
    """
    
    final_markdown_en = call_gemini(step3_en_prompt)
    final_markdown_en = final_markdown_en.replace("```markdown\n", "").replace("```\n", "").strip()
    
    # ì •ì œ í•¨ìˆ˜ ì ìš©
    final_markdown_en = clean_generated_text(final_markdown_en)

    print(f"ğŸš€ [4/5] í•œê¸€ ë²„ì „ ë§ˆí¬ë‹¤ìš´ ë²ˆì—­ ì¤‘ (ì „ë¬¸ê°€ í†¤ì•¤ë§¤ë„ˆ ì ìš©)...")
    step4_ko_prompt = f"""
    ë‹¤ìŒì€ ë°©ê¸ˆ ì‘ì„±ëœ ì˜ë¬¸ ê¸°ìˆ  ë¸”ë¡œê·¸ ë§ˆí¬ë‹¤ìš´ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    ì´ ë‚´ìš©ì„ í•œêµ­ì–´ ë¸”ë¡œê·¸ ë…ì(ê²Œì„ í´ë¼ì´ì–¸íŠ¸ í”„ë¡œê·¸ë˜ë¨¸ ë° AI ì—”ì§€ë‹ˆì–´)ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ì½ì„ ìˆ˜ ìˆë„ë¡ ë²ˆì—­í•´ì£¼ì„¸ìš”.

    [ë²ˆì—­ í†¤ì•¤ë§¤ë„ˆ ê°€ì´ë“œ]
    1. ë¬¸ì²´: ë„ì…ë¶€ì™€ ë§ºìŒë§ì€ ì „ë¬¸ì ì´ê³  ê¹”ë”í•œ ê²½ì–´ì²´(~í•©ë‹ˆë‹¤, ~ìŠµë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•˜ê³ , ê° ê¸°ì‚¬ì˜ ìš”ì•½ í•­ëª©(í•µì‹¬ ë‚´ìš©, ê¸°ìˆ ì  ì˜ë¯¸, í™œìš© ë°©ì•ˆ)ì€ ê°„ê²°í•œ ëª…ì‚¬í˜• ë˜ëŠ” ê°œì¡°ì‹(~í•¨, ~ì„)ìœ¼ë¡œ ëë§ºìœ¼ì„¸ìš”.
    2. ì „ë¬¸ ìš©ì–´: Rendering, Overhead, Fine-tuning, LLM ë“± ê²Œì„ ë° AI ì—…ê³„ì—ì„œ í”íˆ ì“°ì´ëŠ” ê¸°ìˆ  ìš©ì–´ëŠ” ì–µì§€ë¡œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì§€ ë§ê³  ì˜ë¬¸ ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜ ìµìˆ™í•œ ì—…ê³„ ìš©ì–´(ìŒì—­)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    3. ì–´ì¡°: ê³¼ì¥ëœ ìˆ˜ì‹ì–´ë¥¼ ë°°ì œí•˜ê³  ê°ê´€ì ì´ê³  ê¸°ìˆ  ì¤‘ì‹¬ì ì¸ ì‹œê°ì„ ìœ ì§€í•˜ì„¸ìš”.

    ë§ˆí¬ë‹¤ìš´ í˜•ì‹(Frontmatter í¬í•¨)ê³¼ ë§í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , ë³¸ë¬¸ ë‚´ìš©ë§Œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. Frontmatterì˜ titleê³¼ descriptionë„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.
    ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```markdown ë“±) ê¸°í˜¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

    [ì˜ë¬¸ í¬ìŠ¤íŠ¸]
    {final_markdown_en}
    """
    
    final_markdown_ko = call_gemini(step4_ko_prompt)
    final_markdown_ko = final_markdown_ko.replace("```markdown\n", "").replace("```\n", "").strip()
    
    # ì •ì œ í•¨ìˆ˜ í•œë²ˆ ë” ì ìš© (ë²ˆì—­ ê³¼ì •ì—ì„œ ìƒê¸¸ ìˆ˜ ìˆëŠ” ì˜¤ë¥˜ ë°©ì§€)
    final_markdown_ko = clean_generated_text(final_markdown_ko)

    print(f"ğŸš€ [5/5] íŒŒì¼ ì €ì¥ ì¤‘...")
    target_dir = os.path.join(TARGET_REPO_PATH, "content", "journal")
    os.makedirs(target_dir, exist_ok=True)
    
    # ì˜ë¬¸ ë¬¸ì„œ ì €ì¥ (.md)
    file_name_en = f"{TODAY_STR}_news.md"
    file_path_en = os.path.join(target_dir, file_name_en)
    with open(file_path_en, "w", encoding="utf-8") as f:
        f.write(final_markdown_en)
        
    # í•œê¸€ ë¬¸ì„œ ì €ì¥ (.ko.md)
    file_name_ko = f"{TODAY_STR}_news.ko.md"
    file_path_ko = os.path.join(target_dir, file_name_ko)
    with open(file_path_ko, "w", encoding="utf-8") as f:
        f.write(final_markdown_ko)
        
    print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ë‘ ë²„ì „ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\n  - {file_path_en}\n  - {file_path_ko}")

if __name__ == "__main__":
    main()