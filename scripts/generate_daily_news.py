import os
import sys
import time
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
TARGET_REPO_PATH = os.environ.get("TARGET_REPO_PATH", ".")
KST = timezone(timedelta(hours=9))
TODAY = datetime.now(KST)
TODAY_STR = TODAY.strftime("%Y-%m-%d")

if not GEMINI_API_KEY:
    print("ğŸš¨ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

genai.configure(api_key=GEMINI_API_KEY)
# ìµœì‹  Flash ëª¨ë¸ ì‚¬ìš© (Gemini 2.5 Flash)
model = genai.GenerativeModel('gemini-2.5-flash')

# ì•ˆì „ í•„í„° ì™„í™” (ê¸°ìˆ  ë¬¸ì„œ ìš”ì•½ ì‹œ ì˜¤íƒì§€ ë°©ì§€)
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

def call_gemini_with_retry(prompt: str, is_json=False) -> str:
    """API í˜¸ì¶œ ì œí•œ(429) ë“±ì— ëŒ€ë¹„í•œ ì¬ì‹œë„ ë¡œì§"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            generation_config = {"response_mime_type": "application/json"} if is_json else {}
            response = model.generate_content(
                prompt,
                safety_settings=safety_settings,
                generation_config=generation_config
            )
            return response.text
        except Exception as e:
            print(f"    âš ï¸ Gemini API í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(30 * (attempt + 1)) # 30ì´ˆ, 60ì´ˆ ëŒ€ê¸°
            else:
                raise

def fetch_recent_rss_entries() -> list:
    """ìµœê·¼ 24ì‹œê°„ ì´ë‚´ì˜ RSS í”¼ë“œ ìˆ˜ì§‘"""
    urls = [
        "https://news.ycombinator.com/rss",
        "https://www.reddit.com/r/MachineLearning/new/.rss"
    ]
    yesterday = TODAY - timedelta(days=1)
    entries = []

    for url in urls:
        print(f"  ğŸ“¥ RSS íŒŒì‹± ì¤‘: {url}")
        feed = feedparser.parse(url)
        for entry in feed.entries:
            # RSS ë°œí–‰ ì‹œê°„ í™•ì¸ (ì—†ëŠ” ê²½ìš° í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê°„ì£¼)
            published_tuple = entry.get('published_parsed', entry.get('updated_parsed'))
            if published_tuple:
                published_dt = datetime(*published_tuple[:6], tzinfo=timezone.utc)
                if published_dt > yesterday:
                    entries.append({
                        "title": entry.title,
                        "link": entry.link,
                        "summary": entry.get('summary', '')[:200] # ìš”ì•½ë³¸ ì¼ë¶€ë§Œ
                    })
    return entries

def extract_webpage_text(url: str) -> str:
    """URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OpenClawì˜ xurl ì—­í•  ëŒ€ì²´)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.extract()
            
        text = soup.get_text(separator=' ', strip=True)
        return text[:3000] # í† í° ì œí•œì„ ìœ„í•´ ì•ë¶€ë¶„ 3000ìë§Œ ì¶”ì¶œ
    except Exception as e:
        print(f"    âš ï¸ {url} ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def main():
    print(f"ğŸš€ [1/4] RSS í”¼ë“œ ìˆ˜ì§‘ ë° ê¸°ì‚¬ ì„ ë³„ ì‹œì‘...")
    rss_entries = fetch_recent_rss_entries()
    
    if not rss_entries:
        print("ğŸš¨ ìµœê·¼ 24ì‹œê°„ ë‚´ì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # Step 1: ê¸°ì‚¬ ì„ ë³„ (JSON ì‘ë‹µ ê°•ì œ)
    rss_text = "\n".join([f"- ì œëª©: {e['title']}\n  ë§í¬: {e['link']}\n  ìš”ì•½: {e['summary']}" for e in rss_entries])
    step1_prompt = f"""
    ë‹¤ìŒì€ ìµœê·¼ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤.
    ì´ ì¤‘ì—ì„œ 'ê²Œì„ í”„ë¡œê·¸ë˜ë°' ë° 'AI/ML ê¸°ìˆ 'ê³¼ ê´€ë ¨ëœ ê°€ì¥ ì¤‘ìš”í•œ ê¸°ì‚¬ë¥¼ **ìµœëŒ€ 3ê°œë§Œ** ì„ ë³„í•´ì£¼ì„¸ìš”.
    Unreal/Unity ì—…ë°ì´íŠ¸, LLM ë…¼ë¬¸, ê·¸ë˜í”½ìŠ¤ ìµœì í™” ë“± ê¸°ìˆ  ì¤‘ì‹¬ì´ì–´ì•¼ í•˜ë©°, ë‹¨ìˆœ ë¹„ì¦ˆë‹ˆìŠ¤ë‚˜ ê²Œì„ ì¶œì‹œ ì†Œì‹ì€ ì œì™¸í•˜ì„¸ìš”.
    
    ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ì˜ ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
    [
      {{"title": "ê¸°ì‚¬ ì œëª©", "link": "ê¸°ì‚¬ URL"}}, ...
    ]
    
    ê¸°ì‚¬ ëª©ë¡:
    {rss_text}
    """
    selected_links_json = call_gemini_with_retry(step1_prompt, is_json=True)
    selected_articles = json.loads(selected_links_json)
    print(f"    âœ… {len(selected_articles)}ê°œì˜ ê¸°ì‚¬ ì„ ë³„ ì™„ë£Œ.")

    print(f"ğŸš€ [2/4] ì„ ë³„ëœ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½...")
    summaries = []
    for article in selected_articles:
        print(f"    ğŸ“– ë¶„ì„ ì¤‘: {article['title']}")
        content = extract_webpage_text(article['link'])
        
        step2_prompt = f"""
        ë‹¤ìŒ ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        
        ì œëª©: {article['title']}
        ë§í¬: {article['link']}
        ë³¸ë¬¸ ë‚´ìš©: {content if content else "(ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ë§í¬ ê¸°ë°˜ìœ¼ë¡œ ìœ ì¶”í•˜ì„¸ìš”.)"}
        
        í˜•ì‹:
        #### ê¸°ì‚¬
        ë§í¬: [{article['title']}]({article['link']})
        ìš”ì•½: (í•µì‹¬ ê¸°ìˆ  ë‚´ìš© 1ì¤„)
        ì˜í–¥: (ê²Œì„/AI ê°œë°œ ì˜í–¥ 1ì¤„)
        """
        summary = call_gemini_with_retry(step2_prompt)
        summaries.append(summary)
    
    print(f"ğŸš€ [3/4] ìµœì¢… ë§ˆí¬ë‹¤ìš´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±...")
    combined_summaries = "\n\n".join(summaries)
    step3_prompt = f"""
    ì˜¤ëŠ˜ ë‚ ì§œëŠ” {TODAY_STR}ì…ë‹ˆë‹¤.
    
    ë‹¤ìŒ ìš”ì•½ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    íƒ€ê¹ƒ ë…ì: 'ê²Œì„ í´ë¼ì´ì–¸íŠ¸ í”„ë¡œê·¸ë˜ë¨¸' ë° 'AI ì—”ì§€ë‹ˆì–´'.
    
    [ì¶œë ¥ í˜•ì‹]
    ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ìš© ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ë§Œ ì¶œë ¥. ì¶”ê°€ ì„¤ëª…/ì½”ë“œë¸”ë¡(```markdown ë“±) ê¸ˆì§€.
    ì‹¤ì œ ê¸°ì‚¬ URL ë§í¬ í•„ìˆ˜.
    
    ---
    title: "[ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§¤ë ¥ì ì¸ ì œëª© ì‘ì„± - ì˜ˆ: {TODAY_STR} Unreal C++ ìµœì í™” & LLM íŠ¸ë Œë“œ]"
    date: {TODAY.strftime("%Y-%m-%dT09:00:00+09:00")}
    draft: false
    description: "[í•µì‹¬ ê¸°ìˆ  ë™í–¥ 2-3ì¤„ ìš”ì•½ - ê²Œì„ ê°œë°œ/AI ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸ ì¤‘ì‹¬]"
    tags: ["News", "Game Programming", "AI Trends", "Tech"]
    categories: ["Tech"]
    ---
    
    ìµœì‹  ê²Œì„ í”„ë¡œê·¸ë˜ë° ë° AI ê¸°ìˆ  ë™í–¥ì„ ì „í•´ë“œë¦½ë‹ˆë‹¤.
    
    (ì´í›„ ê° ê¸°ì‚¬ë³„ë¡œ ì•„ë˜ í˜•ì‹ ìœ ì§€)
    #### 1. [ì‹¤ì œ ê¸°ì‚¬ ì œëª©](ì‹¤ì œ ë§í¬ URL)
    * **í•µì‹¬ ë‚´ìš©:** ...
    * **ê¸°ìˆ ì  ì˜ë¯¸:** ...
    * **í™œìš© ë°©ì•ˆ:** ...
    
    [ìš”ì•½ ë°ì´í„°]
    {combined_summaries}
    """
    
    final_markdown = call_gemini_with_retry(step3_prompt)
    
    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ë§ˆì»¤ê°€ ì„ì—¬ ë“¤ì–´ì˜¬ ê²½ìš° ì œê±°
    final_markdown = final_markdown.replace("```markdown\n", "").replace("```\n", "").strip()

    print(f"ğŸš€ [4/4] íŒŒì¼ ì €ì¥ ì¤‘...")
    target_dir = os.path.join(TARGET_REPO_PATH, "content", "journal")
    os.makedirs(target_dir, exist_ok=True)
    
    file_name = f"{TODAY_STR}_news.ko.md"
    file_path = os.path.join(target_dir, file_name)
    
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(final_markdown)
        
    print(f"ğŸ‰ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")

if __name__ == "__main__":
    main()